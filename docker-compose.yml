# This file defines all the services (containers) that make up our application.
# We can start, stop, and manage all of them with a single command.

version: '3.8'

services:
  # n8n: Our low-code workflow automation and AI orchestration tool.
  n8n:
    build:
      context: ./n8n_custom
    container_name: n8n_service
    restart: unless-stopped
    # Use the host's network directly. This can solve complex connection issues.
    network_mode: host
    volumes:
      - ./n8n_data:/home/node/.n8n
      - ./scripts:/scripts
      - ./session_data:/session_data
    environment:
      # In host mode, n8n can reach the host machine (and VcXsrv) at localhost.
      - DISPLAY=host.docker.internal:0.0
      # The webhook URL must also be updated for host mode.
      - WEBHOOK_URL=http://localhost:5678
      - N8N_DISABLE_PRODUCTION_MAIN_PROCESS=true

  # Ollama: Our local Large Language Model server.
  ollama:
    image: ollama/ollama
    container_name: ollama_service
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "11434:11434"
    volumes:
      - ./ollama_data:/root/.ollama
    networks:
      - finance-net

networks:
  # The n8n service no longer uses this, but Ollama still does.
  finance-net:
    driver: bridge
